[
  {
    "objectID": "posts/wakefield/index.html",
    "href": "posts/wakefield/index.html",
    "title": "Wakefield’s ABF vs Laplace approximation for variable selection with SuSiE",
    "section": "",
    "text": "Blah"
  },
  {
    "objectID": "posts/logistic_finemapping/index.html",
    "href": "posts/logistic_finemapping/index.html",
    "title": "Fine-mapping case contral GWAS",
    "section": "",
    "text": "Case-control GWAS are common place. Variants are analyzed one at a time. Often a linear model or linear mixed model are fit to case control GWAS, which is justified in the case of small effect (Pirinen, Donnelly, and Spencer 2013), and summaries of individual associations with each SNP are reported. In other cases a logistic regression is fit, or even a generalized linear mixed model.\nGenetic fine-mapping methods, broadly, aim to estimate a sparse joint model of effects across all SNPs. Many fine-mapping methods are developed to infer the sparse joint model from the marginal association summaries (Guan and Stephens 2011) (Zou et al. 2022). These approaches assume that the marginal effect sizes or \\(z\\)-scores are multivariate normal with a mean determined by the (unobserved) true causal effects, and a variance dependent on the LD between SNPs.\n\\[\\begin{align}\n\\hat \\beta \\sim N(SRS^{-1} {\\bf b}, R)\n\\end{align}\\]\nHowever, often we do not have access to the in-sample LD matrix, and it is replaced with an estimate of the LD from a reference panel \\(\\hat R\\). Furthermore, while the covariance of marginal effect estimates is given by the in-sample LD for regular linear regression, it is not the case for logistic regression. We want to better understand what the best analysis option is for case control GWAS. Is it appropriate to analyze case-control GWAS with logistic regression, and analyze the resulting summary statistics with the usual fine-mapping methods? Is it better to perform linear GWAS?\nApplying SuSiE RSS to logistic regression summary statistics is closely related to Wakefieldls approximation in the case where we assume there is one causal variant. We are aware that using Wakefields approximation is a problem when the \\(z\\)-scores are large, because the ABFs are based on an approximation of the likelihood ratio that constrains the \\(\\widehat{LR}(0) = 1\\), even though the approximation may be poor in the tail. Variation in the accuracy of Wakefield’s approximation across different variables can influence inferences about which variables to include in the model.\nIn this document we explore the behavior of applying SuSiE-RSS to summary statistics from logistic regression and linear regression. We consider the cases where"
  },
  {
    "objectID": "posts/logistic_finemapping/index.html#small-effect-large-z-score",
    "href": "posts/logistic_finemapping/index.html#small-effect-large-z-score",
    "title": "Fine-mapping case contral GWAS",
    "section": "Small effect, large \\(z\\)-score",
    "text": "Small effect, large \\(z\\)-score\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\n\nsigmoid <- function(x){1 / (1 + exp(-x))}\nlogit <- function(p){log(p) - log(1-p)}\n\nlogsumexp <- function(x){\n  c <- max(x)\n  return(c + log(sum(exp(x - c))))\n}\n\ncompute_cs <- function(alpha, target_coverage = 0.95){\n  idx <- order(alpha, decreasing = T)\n  coverage <- cumsum(alpha[idx])\n  size <- which(coverage >= target_coverage)[1]\n  cs <- idx[1:size]\n  alpha <- alpha[cs]\n  coverage <- sum(alpha)\n  return(list(cs=cs, alpha=alpha, size=size, coverage = coverage, target_coverage = target_coverage))\n}\n\n\n\n\nCode\n#' Fit logistic SER\nlogistic_ser_laplace <- function(y, X, prior_variance, maxiter=100, tol=1e-5){\n  f <- function(i){\n    x <- X[, i]\n    return(glm(y ~ x, family='binomial'))\n  }\n  p <- ncol(X)\n  fits <- purrr::map(1:p, f)\n  \n  betahat <- purrr::map_dbl(fits, ~coef(.x)[2])\n  intercept <- purrr::map_dbl(fits, ~coef(.x)[1])\n  sehat <- purrr::map_dbl(fits, ~summary(.x)$coef[2, 2]) # note we actually just want the sqrt(-1/H[2,2]) where H is the hessian at mle\n  ll <- purrr::map_dbl(fits, ~-0.5 * .x$deviance)\n  ll0 <- purrr::map_dbl(fits, ~-0.5 * .x$null.deviance)\n\n  tau0 <- 1/prior_variance\n  tau <- 1/sehat**2\n  mu <- tau/(tau + tau0) * betahat\n  var <- 1 /(tau + tau0)\n  lbf <- (ll - ll0) + 0.5 * log(tau0/(tau + tau0)) - 0.5 * (1/tau + 1/tau0) * betahat**2\n  alpha <- exp(lbf - logsumexp(lbf))\n  psi <- (X %*% (alpha * mu))[, 1] + sum(alpha * intercept)\n  cs <- compute_cs(alpha)\n  return(list(mu = mu, alpha = alpha, lbf = lbf, prior_variance=prior_variance, intercept=intercept, psi=psi, cs=cs, betahat=betahat, sehat=sehat))\n}\n\n#' Compute wakefield SER\nlogistic_ser_wakefield <- function(lapser){\n  lbf <- with(lapser, dnorm(betahat, 0, sd = sqrt(prior_variance + sehat^2), log=T) - dnorm(betahat, 0, sd=sehat, log=T)) # wakefield\n  alpha <- exp(lbf - logsumexp(lbf))\n  mu <- lapser$mu\n  psi <- (X %*% (alpha * mu))[, 1] + sum(alpha * lapser$intercept)\n  cs <- compute_cs(alpha)\n  return(modifyList(lapser, list(lbf=lbf, alpha=alpha, psi=psi, cs=cs)))\n}\n\n\n\n\nCode\nset.seed(12) # decreasing ll\n\nn <- 1000\np <- 100\nX <- matrix(rnorm(n*p), nrow=n)\nx <- X[, 1]\nlogodds <- -2 + x\ny <- as.integer(logodds + rlogis(n) > 0)\n\nser_lap <- logistic_ser_laplace(y, X, 1.0)\nser_abf <- logistic_ser_wakefield(ser_lap)\nplot(ser_abf$lbf, ser_lap$lbf); abline(0, 1, col='red')"
  },
  {
    "objectID": "posts/ser_exploration/profile_likelihood_intercept/index.html",
    "href": "posts/ser_exploration/profile_likelihood_intercept/index.html",
    "title": "Profile likelihood for logistic regression",
    "section": "",
    "text": "How many Newton steps does it take to optimize the intercept?\nConsider the case where \\(y \\sim \\text{Bernoulli}(\\frac{1}{1 + \\exp\\{b_0 + \\psi_i\\}})\\) for known \\(\\psi_i\\). How do we optimize the intercept \\(b_0\\)?\nWrite \\(\\mu_i = \\frac{1}{1 + \\exp\\{b_0 + \\psi_i\\}}\\).\n\\[\n\\begin{align}\nL(b_0) &= \\sum y_i(b_0 + \\psi_i) - \\log(1 + \\exp \\{ b_0 + \\psi_i \\}) \\\\\n\\frac{d}{db_0} L &= \\sum y_i - \\mu_i \\\\\n\\frac{d^2}{db_0} &= -\\sum \\mu_i(1 - \\mu_i)\n\\end{align}\n\\]\n\nn <- 1000\npsi <- rnorm(n) *0\nb0 <- -2\nlogit <- b0 + psi\nprob <- 1/(1 + exp(-logit))\ny <- rbinom(n, 1, prob)\n\nsoltn <- glm(y ~ 1 + offset(psi), family='binomial')$coef\noptimize_b0 <- function(y, psi, b0_init = 0, maxit=10, tol=1e-10){\n  b0 <- b0_init\n  for (i in 1:maxit){\n    mu <- 1 / (1 + exp(-(b0 + psi)))\n    grad <- sum(y - mu)\n    hess <- -sum(mu *(1 - mu))\n    dir <- grad/hess\n    b0 <- b0 - dir\n    nd <- -0.5 * grad^2/hess\n    if(nd < tol){\n      break\n    }\n  }\n  return(list(b0 = b0, iter=i, nd = nd, grad=grad, hess = hess))\n}\nopt <- optimize_b0(y, psi)\nopt\n\n$b0\n[1] -2.197225\n\n$iter\n[1] 6\n\n$nd\n[1] 4.285256e-20\n\n$grad\n[1] -2.777312e-09\n\n$hess\n[1] -90\n\n\n\nf <- Vectorize(function(b0){\n  tmp <- (b0 + psi)\n  return(sum(y * tmp - log(1 + exp(tmp))))\n})\nplot(f, xlim = c(-3, -1))\n\n\n\n\nHere we are going to try and optimize the intercept in a different way. Observe that we need \\(\\sum_i y_i = \\sum_i \\mu_i\\)\nOur approach is to take a quadratic approximation to the RHS.\n\n# simulate\nn <- 1000\npsi <- rnorm(n) *0\nb0 <- -2\nlogit <- b0 + psi\nprob <- 1/(1 + exp(-logit))\ny <- rbinom(n, 1, prob)\n\n\nb00 <- -2\nf <- Vectorize(function(x){sum(1 / (1 + exp(-(x + psi))))})\ng <- Vectorize(function(x){\n  mu <- 1 / (1 + exp(-(b00 + psi)))\n  return(sum(mu) + sum(mu * (1-mu)) * (x-b00))\n})\nh <- Vectorize(function(x){\n  mu <- 1 / (1 + exp(-(b00 + psi)))\n  u = sum(mu)\n  v = sum(mu*(1-mu))\n  w = 0.5 * sum(mu *(1 - mu) * (1 - 2 * mu))\n  return(u + v*(x - b00) + w *(x - b00)^2)\n  # a = w\n  # b = v - 2 * w * b0\n  # c = u - v*b0 + w * b0^2 - m\n  # return(a*x^2 + b*x + c)\n})\nj <- Vectorize(function(x){\n  mu <- 1 / (1 + exp(-(b00 + psi)))\n  u = sum(mu)\n  v = sum(mu*(1-mu))\n  w = sum(mu *(1 - mu) * (1 - 2 * mu))\n  y = \n  return(u + v*(x - b00) + 0.5 * w *(x - b00)^2)\n  # a = w\n  # b = v - 2 * w * b0\n  # c = u - v*b0 + w * b0^2 - m\n  # return(a*x^2 + b*x + c)\n})\n\nplot(f, xlim = c(-3, 2))\nplot(g, xlim = c(-3, 2), add=T, col='red')\nplot(h, xlim = c(-3, 2), add=T, col='blue')\nabline(h=sum(y), lty=3)\n\n\n\n\n\noptimize_b02 <- function(y, psi, b0_init = 0, maxit=10, tol=1e-10){\n  b0 <- b0_init\n  m <- sum(y)\n  \n  for (i in 1:maxit){\n    \n\n    \n    -b + 0.5 * sqrt(b^2 - 4*a*c)/a\n    \n    -b \n    grad <- sum(y - mu)\n    hess <- -sum(mu *(1 - mu))\n    dir <- grad/hess\n    b0 <- b0 - dir\n    nd <- -0.5 * grad^2/hess\n    if(nd < tol){\n      break\n    }\n  }\n  return(list(b0 = b0, iter=i, nd = nd, grad=grad, hess = hess))\n}\n\n\n# simulate\nn <- 1000\nx <- rnorm(n)\nb0 <- -2\nlogit <- b0 + x\nprob <- 1/(1 + exp(-logit))\ny <- rbinom(n, 1, prob)\n\nloglik <- function(y, x, o=0, beta, beta0){\n  psi\n  \n}\nfit <- glm(y ~ x, family='binomial')$coef\n\n\n# make a range of X with varrying correlation with x1\np <- 50\nsd <- 0.5\nX <- matrix(rep(0, n*p), nrow = n)\nX[, 1] <- scale(x)\nfor (i in 2:p){\n  X[, i] <- scale(X[, i-1] +rnorm(n) * sd)\n}\ncor(X[,1], X)[1,]\n\n [1] 1.00000000 0.89328345 0.78020422 0.69966558 0.61124226 0.54096420\n [7] 0.49283510 0.42171327 0.36985073 0.32645077 0.29274502 0.28425543\n[13] 0.27459786 0.25395398 0.24257115 0.19612442 0.17855252 0.18012802\n[19] 0.15640339 0.12763770 0.11730547 0.10559813 0.08327092 0.06981223\n[25] 0.07292970 0.05361353 0.03854335 0.02085034 0.03569222 0.05058369\n[31] 0.06606282 0.07003180 0.03699457 0.06023426 0.07240543 0.06886601\n[37] 0.06391152 0.07555611 0.08908833 0.05274430 0.03545549 0.03608596\n[43] 0.02554683 0.01030323 0.01840938 0.03481289 0.05666265 0.03978949\n[49] 0.03293163 0.02520362\n\n\nIt is easy to optimize the intercept via Newton’s method."
  },
  {
    "objectID": "posts/ser_exploration/index.html",
    "href": "posts/ser_exploration/index.html",
    "title": "One variable at a time is better for inference in SER",
    "section": "",
    "text": "Variable selection is hard. There is a special case of variable selection that is easy.\nThe natural approach for fitting the SER is to fit the univariate regression for each variable separately, and then compare the univariate models to perform inference on which model is most likely.\nIRLS and variational approximations for GLMs generally take a different approach. For each observation we make a local, quadratic approximation to the log likelihood. In the case of backfitting, we take a Taylor approximation about the current estimate of the linear predictor. In the case of variational approximations, we maximize the expected likelihood over a family of variational lower bounds. While the IRLS approach is available to all GLMs, it does not take into account uncertainty in the linear predictors. In contrast, the variational approximations account for uncertainty, but are tailor made for specific observation models, and are less widely applicable.\nIn this post we highlight a limitation of both approaches with respect to variable selection problems. We focus on the simple case of a single effect regression (SER) where inference is particularly straight forward. We show that performing approximate inference in the SER we get undesirable behavior: credible sets from our approximate posterior systematically undercover the causal effect variable."
  },
  {
    "objectID": "posts/ser_exploration/index.html#methods",
    "href": "posts/ser_exploration/index.html#methods",
    "title": "One variable at a time is better for inference in SER",
    "section": "Methods",
    "text": "Methods\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\n\nsigmoid <- function(x){1 / (1 + exp(-x))}\nlogit <- function(p){log(p) - log(1-p)}\n\nlogsumexp <- function(x){\n  c <- max(x)\n  return(c + log(sum(exp(x - c))))\n}\n\ncompute_cs <- function(alpha, target_coverage = 0.95){\n  idx <- order(alpha, decreasing = T)\n  coverage <- cumsum(alpha[idx])\n  size <- which(coverage >= target_coverage)[1]\n  cs <- idx[1:size]\n  alpha <- alpha[cs]\n  coverage <- sum(alpha)\n  return(list(cs=cs, alpha=alpha, size=size, coverage = coverage, target_coverage = target_coverage))\n}\n\n\n\nLaplace approximation\nThe GLM-SER is simply a GLM with an SER prior on the effect vector. Estimating the SER with a fixed prior variance is similarly straightforward. Once we condition on which variable is included, the problem reduces to \\(p\\) separate regression problems. The algorithm is simple: fit \\(p\\) seperate univariate regressions to get the posterior \\(p(b | \\gamma =j, {\\bf x}_j, {\\bf y})\\) and normalize the BFs to get \\(p(\\gamma)\\).\nIn this post the method we call “Laplace approximation” approximates the posteriore \\(p(b | \\gamma, y, X)\\) using the Laplace approximation. The key feature that distinguishes this approach from the other approaches presented here is that each variable is treated separately. The fact that we use a Laplace approximation for each variable, rather than e.g. a variational approximation is of little importance, we assume that the approximation is close to exact.\n\n\nCode\n#' Fit logistic SER\nlogistic_ser_laplace <- function(y, X, prior_variance, maxiter=100, tol=1e-5){\n  f <- function(i){\n    x <- X[, i]\n    return(glm(y ~ x, family='binomial'))\n  }\n  p <- ncol(X)\n  fits <- purrr::map(1:p, f)\n  \n  betahat <- purrr::map_dbl(fits, ~coef(.x)[2])\n  intercept <- purrr::map_dbl(fits, ~coef(.x)[1])\n  sehat <- purrr::map_dbl(fits, ~summary(.x)$coef[2, 2]) # note we actually just want the sqrt(-1/H[2,2]) where H is the hessian at mle\n  ll <- purrr::map_dbl(fits, ~-0.5 * .x$deviance)\n  ll0 <- purrr::map_dbl(fits, ~-0.5 * .x$null.deviance)\n\n  tau0 <- 1/prior_variance\n  tau <- 1/sehat**2\n  mu <- tau/(tau + tau0) * betahat\n  var <- 1 /(tau + tau0)\n  lbf <- (ll - ll0) + 0.5 * log(tau0/(tau + tau0)) - 0.5 * (1/tau + 1/tau0) * betahat**2\n  alpha <- exp(lbf - logsumexp(lbf))\n  psi <- (X %*% (alpha * mu))[, 1] + sum(alpha * intercept)\n  cs <- compute_cs(alpha)\n  return(list(mu = mu, alpha = alpha, lbf = lbf, prior_variance=prior_variance, intercept=intercept, psi=psi, cs=cs))\n}\n\n\n\n\nIteratively reweighted least squares\nGLMs are often fit via Newton’s method. When we run Newton’s method with a fixed stepsize of \\(1\\), the Newton updates can be rewritten as a weighted least squares problem. You will often hear that GLMs are fit by iteratively reweighted least squares (IRLS).\n\\[\n\\begin{align}\n\\beta^{+} = \\beta - \\nabla^2 l(\\beta)^{-1} \\nabla l(\\beta)\n\\end{align}\n\\]\nThis update is easily derived by taking a 2nd order Taylor expansion of \\(l\\) about \\(\\beta\\), and optimizing the resulting quadratic approximation.\nIf we use the canonical link function, the GLM has that the natural parameter is a linear function of our covariates \\(X\\), \\(\\eta_i = {\\bf x}_i^T \\beta\\) and \\(\\log p(y_i | \\beta, {\\bf x}_i) = y \\eta - A(\\eta) + \\phi(y)\\). E.g. for logistic regression \\(A(\\eta) = \\log(1 + \\exp(\\eta))\\). Exponential families have the convenient property that \\(A^{'}(\\eta) = \\mathbb E_\\eta[Y] = \\mu\\). We can compute the gradient and Hessian:\n\\[\n\\begin{align}\n\\nabla_\\beta l_i(\\beta) &= y_i {\\bf x}_i - A^{'}(\\eta_i){\\bf x}_i = (y_i - \\mu_i) {\\bf x}_i, \\\\\n\\nabla_\\beta l(\\beta) &= X^T ({\\bf y} - \\mu)\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\nabla^2_\\beta l_i(\\beta) &= - A^{''}(\\eta_i) {\\bf x}_i {\\bf x}_i^T, \\\\\n\\nabla^2_\\beta l(\\beta) &= - X^T W X\n\\end{align}\n\\]\nWhere \\(W = \\text{diag}((A^{''}(\\eta_i))_{i=1}^n)\\)\nNow we will rewrite our newton update rule:\n\\[\n\\begin{align}\n\\beta^{+} &= \\beta - \\nabla^2 l(\\beta)^{-1} \\nabla l(\\beta) \\\\\n&= \\beta + (X^T W X)^{-1} X^T ({\\bf y} - \\mu) \\\\\n&= \\beta + (X^T W X)^{-1} X^T ({\\bf y} - \\mu) \\\\\n&= (X^T W X)^{-1} X^T W X \\beta + (X^T W X)^{-1} X^T W W^{-1} ({\\bf y} - \\mu) \\\\\n&= (X^T W X)^{-1} X^T W (X \\beta +  W^{-1} ({\\bf y} - \\mu)) \\\\\n&= (X^T W X)^{-1} X^T W (\\eta +  W^{-1} ({\\bf y} - \\mu)) \\\\\n\\end{align}\n\\]\nSo we have shown that the Newton update \\(\\beta^+\\) is the solution to the weighted least squares problem with adjusted data \\({\\bf z} = \\eta + W^{-1}(\\bf y - \\mu)\\) and weights \\(W\\). Iteratively reweighted least squares proceeds by iteratively computing weights and adjusted data and solving the WLS problem. It is plain to see that this is just perform Newton’s method with a step size of \\(1\\).\n\n\nCode\n#' form adjusted data for logistic regression\nmake_adjusted_data <- function(y, psi){\n  mu <- sigmoid(psi)\n  var <- mu * (1 - mu)\n  z = psi + ((y - mu) / var)\n  return(list(z = z, var = var))\n}\n\n#' WLS just to check\nwls <- function(y, x, weights){\n  n <- length(x)\n  X <- cbind(rep(1, n), x)\n  WX <- cbind(weights, weights*x)\n  tmp <- (t(X) %*% (y * weights))[, 1]\n  solve(t(X) %*% WX, tmp)\n}\n\n#' WLS just to check\ncompute_newton_direction <- function(y, x, psi){\n  mu <- sigmoid(psi)\n  var <- mu * (1 - mu)\n  weights <- var\n  z <- (y - mu)\n  \n  n <- length(x)\n  X <- cbind(rep(1, n), x)\n  WX <- cbind(weights, weights*x)\n  nd <- solve(t(X) %*% WX, t(X) %*% z)[,1]\n  return(nd)\n}\n\n\n\n\nCode\nn <- 1000\np <- 10\nX <- matrix(rnorm(n*p), nrow=n)\nx <- X[, 1]\nlogodds <- -2 + x\ny <- as.integer(logodds + rlogis(n) > 0)\n\nbeta <- coef(glm(y ~ x + 1, family='binomial'))\npsi <- predict(glm(y ~ x + 1, family='binomial'))\nadjusted_data <- make_adjusted_data(y, psi)\n\n# newton update and WLS agree\nwith(adjusted_data, wls(z, x, var))\n\n\n  weights           \n-1.928730  1.026304 \n\n\nCode\nbeta - compute_newton_direction(y, x, psi)\n\n\n(Intercept)           x \n  -1.928730    1.026304 \n\n\n\n\nCode\nn <- 1000\np <- 10\nX <- matrix(rnorm(n*p), nrow=n)\nx <- X[, 1]\nlogodds <- -2 + x\ny <- as.integer(logodds + rlogis(n) > 0)\n\n# fit via glm\nsummary(glm(y ~ x + 1 , family='binomial'))$coef\n\n\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept) -1.8876589  0.1050184 -17.974549 3.083635e-72\nx            0.9450638  0.1020115   9.264292 1.963764e-20\n\n\nCode\n# WLS using predictions at MLE-- \n# results of WLS should agree with GLM, but there is a discrepency\npsi <- predict(glm(y ~ x + 1, family='binomial'))\nadjusted_data <- make_adjusted_data(y, psi)\n\n# wls implementation is correct AND agrees with glm\nwith(adjusted_data, summary(lm(z ~ x + 1, weights=var))$coef)\n\n\n              Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) -1.8876589  0.1039571 -18.158055 6.922544e-64\nx            0.9450638  0.1009805   9.358875 5.167002e-20\n\n\nCode\nwith(adjusted_data, wls(z, x, var))\n\n\n   weights            \n-1.8876589  0.9450638 \n\n\n\n\nCode\n#' Fit weighted SER\nweighted_ser <- function(y, X, weights, prior_variance=1){\n  p <- ncol(X)\n  \n  f <- function(i){\n    yaug <- c(y, c(1, 1, 0, 0))\n    xaug <- c(X[,i], c(1, 0, 1, 0))\n    weightsaug <- c(weights, rep(0.25, 4))\n    return(lm(yaug ~ xaug, weights = weightsaug))\n  }\n  \n  fits <- purrr::map(1:p, ~f(.x))\n  \n  intercepts <- purrr::map_dbl(1:p, ~fits[[.x]]$coef[1])\n  means <- purrr::map_dbl(1:p, ~fits[[.x]]$coef[2])\n  std <- purrr::map_dbl(1:p, ~summary(fits[[.x]])$coef[2, 2])\n  \n  tau <- 1/(std**2)\n  tau0 <- 1/prior_variance\n  mu <- tau/(tau + tau0) * means\n  var <- 1/(tau + tau0)\n  lbf <- dnorm(means, sd = sqrt(std**2 + prior_variance), log=T) - dnorm(means, sd = std, log=T)\n  alpha <- exp(lbf - log(sum(exp(lbf - max(lbf)))) - max(lbf))\n  psi <- sum(intercepts * alpha) + (X %*% (mu * alpha))[, 1]\n  cs <- compute_cs(alpha)\n  return(list(mu = mu, alpha = alpha, lbf = lbf, prior_variance=prior_variance, intercept=intercepts, psi=psi, cs=cs))\n}\n\n#' Fit logistic SER, IRLS inspired approach\nlogistic_ser_irls <- function(y, X, prior_variance, maxiter=100, tol=1e-5){\n  psi <- rep(logit(mean(y)), length(y))\n  ll <- vector('numeric', length = maxiter)\n  for (i in 1:maxiter){\n    adjusted_data <- make_adjusted_data(y, psi)\n    serfit <- with(adjusted_data, weighted_ser(z, X, var, prior_variance))\n    psi <- serfit$psi\n    ll[i] <- sum(y*psi - log(1 + exp(psi)))\n    if(i > 1){\n      if (abs(ll[i] - ll[i-1]) < tol){\n        break\n      }\n    }\n  }\n  ll <- head(ll, i)\n  serfit$ll <- ll\n  serfit$niter <- i\n  serfit$maxiter <- maxiter\n  return(serfit)\n}\n\n\n\n\nVariational approximation\nA key challenge in Bayesian logistic regression is integrating over a non-conjugate prior. This is commonly dealt with via variational techniques. Jaakkola and Jordan propose a quadratic variational lower bound (Jaakkola and Jordan, n.d.) for the logistic log likelihood. For each observation, a variational parameter is optimized to maximize the approximate marginal likelihood. This variational approach corresponds to Polya-Gamma augmentation (Polson, Scott, and Windle 2013). From this perspective, the variational parameter gives the variational approximation of the posterior of a latent variable which renders the model conditionally conjugate. This approach has the been extended to the class of super-Gaussian models (Wenzel et al. 2018).\n\n\nCode\n#' polya Gamma Mean\npg_mean <- function(b, c) {\n  mu <- Matrix::drop(0.5 * b / c * tanh(c / 2))\n  \n  # deal with case of c = 0 mean is b/4\n  idx <- is.na(mu) # does indexing work form matrix entries?\n  mu[idx] <- b[idx] / 4\n  return(mu)\n}\n\n#' KL Divergence between two normal distributions\nnormal_kl <- function(mu, var, mu0 = 0., var0 = 1.) {\n  kl <- 0.5 * (log(var0) - log(var) + var / var0 + (mu - mu0)^2 / var0 - 1)\n  return(kl)\n}\n\n#' KL Divergence between two categorical distributions\ncategorical_kl <- function(alpha, pi) {\n  idx <- alpha > 1 / (length(alpha) * 1e3)\n  sum((alpha * (log(alpha) - log(pi))), na.rm = TRUE)\n}\n\n#' KL Divergence of SER posterior and SER prior\ncompute_ser_kl <- function(alpha, pi, mu, var, mu0, var0) {\n  kl <- categorical_kl(alpha, pi)\n  kl <- kl + sum(alpha * normal_kl(\n    mu, var, mu0, var0\n  ))\n  return(kl)\n}\n\n#' Elbo for logistic SER with JJ approximation\ncompute_elbo <- function(y, X, xi, mu, var, delta, alpha, prior_variance, pi){\n  # jj bound\n  kappa <- y - 0.5\n  b <- mu * alpha\n  psi <- Matrix::drop(X %*% mu) + delta\n  bound <- log(sigmoid(xi)) + (kappa * psi) - (0.5 * xi)\n  \n  # KL[q(b) || p (b)]\n  kl <- compute_ser_kl(alpha, pi, mu, var, 0, prior_variance)\n  return(sum(bound) - kl)\n}\n\nis_monotone <- function(x){\n  return(all(tail(x, -1) - head(x, -1) >= 0))\n}\n\nlogistic_ser_vb <- function(y, X, prior_variance=1.0, maxiter=100, tol=1e-5){\n  # initialize variational params\n  n <- length(y)\n  p <- ncol(X)\n  xi <- rep(0.01, n)\n  mu <- rep(0, p)\n  var <- rep(1, p)\n  alpha <- rep(1/p, p)\n  delta <- logit(mean(y))\n  \n  # fixed\n  kappa <- (y - 0.5)\n  tau0 <- 1. / prior_variance\n  X2 <- X^2\n  Z <- matrix(rep(1, n), nrow=n)\n  pi <- rep(1/p, p)\n  \n  # iter\n  elbo <- rep(0, maxiter)\n  for(iter in 1:maxiter){\n    omega <- pg_mean(1, xi)\n    tmp <- kappa - omega * delta\n    nu <- Matrix::drop(tmp %*% X)\n    tau <- Matrix::drop(omega %*% X2)\n    \n    mu = nu / tau\n    var = 1 / tau\n    \n    logits <- log(pi) - 0.5 * log(tau) + 0.5 * nu^2 / tau\n    logits <- logits - logsumexp(logits)\n    alpha <- exp(logits)\n    \n    #intercept \n    b <- alpha * mu\n    Xb <-  Matrix::drop(X %*% b)\n    delta <- Matrix::drop(Matrix::solve((omega * t(Z)) %*% Z, t(Z) %*% (kappa - omega * Xb)))\n    \n    # compute psi2\n    b2 <- alpha * (mu^2 + var)\n    Xb2 <- Matrix::drop((X2) %*% b2)\n    VXb <- Xb2 - Xb^2\n    psi2 <- (Xb + delta)^2 + VXb\n    #psi2 <- Xb2 + (2 * Xb * delta) + delta^2\n    xi <- sqrt(abs(psi2))\n    \n    elbo[iter] <- compute_elbo(y, X, xi, mu, var, delta, alpha, prior_variance, pi)\n    \n    if (iter > 2){\n      if (diff(tail(head(elbo, iter), 2)) < tol){\n        break\n      }\n    }\n  }\n  cs <- compute_cs(alpha)\n  psi <- Matrix::drop(X %*% b) + delta\n  return(list(mu = mu, alpha = alpha, xi=xi, lbf = 0, prior_variance=prior_variance, intercept=delta, psi=psi, cs=cs, elbo=elbo, iter=iter))\n}"
  },
  {
    "objectID": "posts/ser_exploration/index.html#examples",
    "href": "posts/ser_exploration/index.html#examples",
    "title": "One variable at a time is better for inference in SER",
    "section": "Examples",
    "text": "Examples\n\nOne causal variable, uncorrelated X\n\n\nCode\nset.seed(3) # alternating ll\nset.seed(4) # good\nset.seed(11) # decreasing ll\nset.seed(12) # decreasing ll\n\nn <- 1000\np <- 10\nX <- matrix(rnorm(n*p), nrow=n)\nx <- X[, 1]\nlogodds <- -2 + x\ny <- as.integer(logodds + rlogis(n) > 0)\n\nlogisticserfit <- logistic_ser_irls(y, X, 1.0)\nlserlapfit <- logistic_ser_laplace(y, X, 1.0)\nvbser <- logistic_ser_vb(y, X, 1.)\n\npar(mfrow = c(1, 3))\nplot(logisticserfit$psi, logodds, main='IRLS-SER'); abline(0, 1, col='red')\nplot(lserlapfit$psi, logodds, main='SER'); abline(0, 1, col='red')\nplot(vbser$psi, logodds, main='SER'); abline(0, 1, col='red')\n\n\n\n\n\n\n\nCorrelated X\nIn the following example we simulate under a logistic SER model with causal variable \\(50\\), and correlated design \\(X\\). The logistic SER fit via IRLS reports a CS \\(\\{ 49 \\}\\). Meanwhile, the logistic SER fit via the Laplace approximation reports a CS of \\(\\{49, 50\\}\\). If we repeat this experiment 50 times, we see that the logistic SER fit via IRLS systematically undercovers. Our nomial 95% credible attain a coverge of only \\(84\\%\\), meanwhile the Laplace SER attains the target coverage.\nHere is the code for generating the simulations:\n\n\nCode\nmake_prec <- function (n, rho){\n  prec <- diag(rep(1, n))\n  for (i in 1:(n-1)){\n    prec[i, i+1] <- rho\n    prec[i+1, i] <- rho\n  }\n  return(prec)\n}\n\n#' Simulate a single causal variant with mvn X, correlation determined by ls\nsim_correlated_x <- function(n = 1000, p = 100, ls = 10, idx=as.integer(p/2)){\n  Z <- matrix(rnorm(n*p), nrow=n)\n  mix <- exp(-0.5 * (outer(1:p, 1:p, '-')/ls)**2)\n  X <- Z %*% mix\n  x <- X[, idx]\n  logodds <- -2 + x\n  y <- as.integer(logodds + rlogis(n) > 0)\n  sim <- list(X=X, y=y, logodds = logodds)\n  \n  logisticserfit <- logistic_ser_irls(y, X, 1.0)\n  lserlapfit <- logistic_ser_laplace(y, X, 1.0)\n  vbser <- logistic_ser_vb(y, X, 1.0)\n  \n  irls_cs <- logisticserfit$cs\n  lap_cs <- lserlapfit$cs\n  vb_cs <- vbser$cs\n  \n  irls_covered <- idx %in% irls_cs$cs\n  lap_covered <- idx %in% lap_cs$cs\n  vb_covered <- idx %in% vb_cs$cs\n\n  return(list(\n    irls_ser = logisticserfit, irls_cs = irls_cs, irls_covered = irls_covered,\n    lap_ser =  lserlapfit, lap_cs = lap_cs, lap_covered = lap_covered,\n    vb_ser = vbser, vb_cs = vb_cs, vb_covered = vb_covered,\n    sim = sim))\n}\n\n\nHere is an example where the IRLS, VB, and Laplace methods report different credible sets. IRLS and VB each rely on a global approximation of the log likelihood for all variables. In contrast, the Laplace approach uses a separate approximation for each variable separately. When performing inference in the SER it is important to have accurate estimates of the relative evidence for inclusion of each variable. The problem with the global approximation approach is that the global approximation favors the variables with the strongest evidence.\n\n\nCode\nset.seed(49)\nsim <- sim_correlated_x()\nsim$irls_cs\n\n\n$cs\n[1] 49\n\n$alpha\n[1] 0.9874565\n\n$size\n[1] 1\n\n$coverage\n[1] 0.9874565\n\n$target_coverage\n[1] 0.95\n\n\nCode\nsim$vb_cs\n\n\n$cs\n[1] 49\n\n$alpha\n[1] 0.9965328\n\n$size\n[1] 1\n\n$coverage\n[1] 0.9965328\n\n$target_coverage\n[1] 0.95\n\n\nCode\nsim$lap_cs\n\n\n$cs\n[1] 49 50\n\n$alpha\n[1] 0.93777606 0.03519582\n\n$size\n[1] 2\n\n$coverage\n[1] 0.9729719\n\n$target_coverage\n[1] 0.95\n\n\n\n\nCode\nplot(sim$irls_ser$lbf, 0.5*sim$lap_ser$lbf, xlab='IRLS', ylab='Laplace'); \nabline(0, 1, col='red')\n\n\n\n\n\n\n\nCode\nsim1 <- xfun::cache_rds({\n  set.seed(12)\n  purrr::map(1:100, ~sim_correlated_x(n=200, p=100, ls=20.)) %>%\n    tibble() %>%\n    tidyr::unnest_wider('.')\n})\n\n\n# plot predicted values against tru logodds\n\n\n\n\nCode\n# show coverage\nsim1 %>% \n  summarize(coverage_irls = mean(irls_covered), \n            coverage_laplace = mean(lap_covered), \n            coverage_vb = mean(vb_covered))\n\n\n# A tibble: 1 × 3\n  coverage_irls coverage_laplace coverage_vb\n          <dbl>            <dbl>       <dbl>\n1          0.83             0.95        0.85\n\n\nCode\nirls <- c(unlist(purrr::map(1:100, ~sim1$irls_ser[[.x]]$psi)))\nvb <- c(unlist(purrr::map(1:100, ~sim1$vb_ser[[.x]]$psi)))\nlap <- c(unlist(purrr::map(1:100, ~sim1$lap_ser[[.x]]$psi)))\nlogodds <- c(unlist(purrr::map(1:100, ~sim1$sim[[.x]]$logodds)))\n\npar(mfrow=c(1, 3))\nplot(irls, logodds, main='IRLS'); abline(0,1, col='red')\nplot(vb, logodds, main='VB'); abline(0,1, col='red')\nplot(lap, logodds, main='Laplace'); abline(0,1, col='red')\n\n\n\n\n\nCode\ncor(cbind(irls, vb, lap, logodds))\n\n\n             irls        vb       lap   logodds\nirls    1.0000000 0.9993184 0.9994550 0.9861171\nvb      0.9993184 1.0000000 0.9998559 0.9822569\nlap     0.9994550 0.9998559 1.0000000 0.9844287\nlogodds 0.9861171 0.9822569 0.9844287 1.0000000\n\n\nDespite all three SER approaches having remarkably similar predictions, Laplace, the one variable at a time approach, has calibrated credible sets. The other methods undercover.\nIf you look at the PIPs you see a different story. In IRLS and VB approaches, both “global” approximations, the PIPs are inflated compared to the one-variable at a time approach.\n\n\nCode\n# show coverage\nsim1 %>% \n  summarize(coverage_irls = mean(irls_covered), \n            coverage_laplace = mean(lap_covered), \n            coverage_vb = mean(vb_covered))\n\n\n# A tibble: 1 × 3\n  coverage_irls coverage_laplace coverage_vb\n          <dbl>            <dbl>       <dbl>\n1          0.83             0.95        0.85\n\n\nCode\nirls <- c(unlist(purrr::map(1:100, ~sim1$irls_ser[[.x]]$alpha)))\nvb <- c(unlist(purrr::map(1:100, ~sim1$vb_ser[[.x]]$alpha)))\nlap <- c(unlist(purrr::map(1:100, ~sim1$lap_ser[[.x]]$alpha)))\n\npar(mfrow=c(1, 3))\nplot(lap, irls, main='IRLS'); abline(0,1, col='red')\nplot(lap, vb, main='VB'); abline(0,1, col='red')\nplot(lap, lap, main='Laplace'); abline(0,1, col='red')\n\n\n\n\n\nCode\ncor(cbind(irls, vb, lap))\n\n\n          irls        vb       lap\nirls 1.0000000 0.9791457 0.9249517\nvb   0.9791457 1.0000000 0.9568826\nlap  0.9249517 0.9568826 1.0000000"
  },
  {
    "objectID": "posts/ser_exploration/index.html#discussion",
    "href": "posts/ser_exploration/index.html#discussion",
    "title": "One variable at a time is better for inference in SER",
    "section": "Discussion",
    "text": "Discussion\nWe highlight a limitation of global approximation methods when it comes to performing inference in the single effect regression. Although the global and one-variable-at-a time approach provide remarkably similar predictions, we see dramatic differences in the PIPs, and consequently the CSs. The PIPs and CSs of the one-variable at a time method are better calibrated because they are informed by more accurate estimates of the evidence for the inclusion of each variable, wherase the global approximation methods tend to favor the variables with the strongest evidence and inflating their PIPs.\nIn Bayesian variable selection, calibrated uncertainty estimates over which variables are selected are of primary importance. We favor the one-variable-at-a-time approach, which provide better estimates of the evidence for each variable separately."
  },
  {
    "objectID": "posts/bayesian_logistic_regression/index.html",
    "href": "posts/bayesian_logistic_regression/index.html",
    "title": "Bayesian logistic regression with a fixed prior variance",
    "section": "",
    "text": "JJ approximation for univariate (also may more easily accomondate inclusion of intercept, uncertainty in offset, etc)\nedit/expand existing sections"
  },
  {
    "objectID": "posts/bayesian_logistic_regression/index.html#setup",
    "href": "posts/bayesian_logistic_regression/index.html#setup",
    "title": "Bayesian logistic regression with a fixed prior variance",
    "section": "Setup",
    "text": "Setup\nHere we consider logistic regression with a single covariate and fixed intercept. \\(\\tau\\) is a fixed prior precision.\n\\[\\begin{align}\ny_i | x_i, b_0 \\sim \\text{Bernoulli}(\\sigma(b_0 + bx_9)) \\\\\nb \\sim N(0, \\tau^{-1})\n\\end{align}\\]\nWhere \\(\\sigma(x) = (1 + \\exp(-x))^{-1}\\) is the sigmoid funciton. Our goal is to compute the marginal likelihood, the posterior mean, and the posterior variance (actually we don’t need the variance…)\n\\[\\begin{align}\np(y | X) &= \\int \\prod p(y_i | x_i, b) p(b) db \\\\\n\\mu_{\\text{post}} &= \\int b \\; p(b | y, X) \\\\\n\\sigma^2_{\\text{post}} &= \\int b^2 p(b | y, X) - \\mu_{\\text{post}}^2\n\\end{align}\\]\nThe integrals cannot be computed in closed form, so we rely on numerical integration or approximation."
  },
  {
    "objectID": "posts/bayesian_logistic_regression/index.html#laplace-approximation",
    "href": "posts/bayesian_logistic_regression/index.html#laplace-approximation",
    "title": "Bayesian logistic regression with a fixed prior variance",
    "section": "Laplace Approximation",
    "text": "Laplace Approximation\n\nStandard Laplace approximation\nThe Laplace approximation takes a quadratic approximation of the log-integrand. So to approximation \\(\\int \\exp f(x) dx\\) we would consider the quadratic approximation \\(\\hat f(x; x_0) = f(x_0) + \\nabla f(x_0) (x - x_0) + \\frac{1}{2}(x - x_0)^T \\nabla^2f(x_0) (x - x_0)\\). We will write \\(\\hat f(x) = \\hat f(x; x^*)\\) where \\(x*\\) is the mode of \\(f\\). In this case \\(\\nabla f(x^*) =0\\) and the first order term is eliminated. Write \\(\\Lambda = -\\nabla^2f(x^*)\\) the approximation can be identified as a Gaussian integral\n\\[\n\\begin{align}\n\\int \\exp f(x) \\approx \\int \\exp \\hat f(x) dx = \\exp f(x^*) (2\\pi)^{p/2}|\\Lambda|^{-\\frac{1}{2}}\n\\end{align}\n\\]\nIn the univariate case this simplifies to \\[\n\\begin{align}\n\\int \\exp f(x) \\approx \\int \\exp \\hat f(x) dx = \\exp f(x^*) \\sqrt{\\frac{2\\pi}{\\lambda}}\n\\end{align}\n\\] where \\(\\lambda = -\\frac{d^2}{dx^2}f(x)\\vert_{x= x^*}\\)\nTo make this explicit for our case, we make\n\\[\\begin{align}\nf(b)\n&= \\log p(y | b) + \\log p(b) \\\\\n&= \\sum y_i (x_i b) - \\log (1 + \\exp(x_i b)) - \\frac{\\tau}{2} b^2 + 0.5 \\times \\log(\\frac{\\tau}{2 \\pi})\n\\end{align}\\]\nWe compute the MAP \\(b^* = \\arg\\max_b f(b)\\) and then \\(\\frac{d^2}{db^2} f(b) = \\sum_i \\sigma(x_ib)\\sigma(-x_i b) x_i^2\\) to yield the Laplace approximation.\n\n\nOther expansion points\nWe discuss other Laplace type approximations. The general idea is to decompose \\(f(x) = g(x) + Q(x)\\) where \\(Q\\) is quadratic, then rather than making a Taylor approximation to \\(f\\) directly, we can make a Taylor approximation of \\(g\\), and combine it with \\(Q\\). In our case we might pick \\(g(b) = \\log p(y | X, b)\\) and \\(Q(b) = \\log \\phi(b; 0, \\tau^{-1})\\). In this case we would expand \\(g\\) around the MLE \\(\\hat b\\):\n\\[\\begin{align}\n\\int \\exp f(b) db\n&= \\int \\exp (g(b) + Q(b)) db \\\\\n&\\approx \\int \\exp \\hat g(b) \\phi(b; 0, \\tau^{-1})) db \\\\\n&= \\exp g(\\hat b) (\\frac{2\\pi}{\\nu})^{\\frac{1}{2}} \\int \\phi(\\hat b; b, \\nu^{-1}) \\phi(b; 0, \\tau^{-1})db\\\\\n&= \\exp g(\\hat b) (\\frac{2\\pi}{\\nu})^{\\frac{1}{2}} \\phi(\\hat b; 0, \\nu^{-1} + \\tau^{-1})\n\\end{align}\\]\nWhere we recognize that the convolution of Gaussians is Gaussian with the sum of the variances.\nThis approximation comes with a natural estimate of the posterior mean and variance, \\(\\mu = \\frac{\\nu}{\\nu + \\tau} \\hat \\beta\\) and \\(\\sigma^2 = (\\nu + \\tau)^{-1}\\).\nWe might also consider the choice\n\\[\\begin{align}\ng(b) = \\log p(y | X, b) - \\log \\phi(b; 0, \\tau_2^{-1}), \\\\\nQ(b) = \\log \\phi(b; 0, \\tau^{-1}) + \\log \\phi(b; 0, \\tau_2^{-1}).\n\\end{align}\\]\nFor \\(\\tau_2\\) small, this might be a good alternative to expanding around the MLE, as we can be assured that the MAP always exists. However, this is probably not a good choice for \\(\\tau_2 >> \\tau\\).\n\n\nWhy expand at a point other than the mode\nThis approach is less relevant when we consider a fixed prior, because it is not harder to compute the MAP than it is to compute the MLE. However, it is of interest how accurate these approximations are across a range of prior variances. If the approximation is good, it provides an inexpensive way of tuning the prior variance– the approximate likelihood can be computed with a single evaluation of the Gaussian density function, rather than re-optimizing for each value of \\(\\tau^{-1}\\). Note: even if we can use this approach to select \\(\\tau\\), it is probably wise to compute a more accurate approximation of the marginal likelihood, posterior mean, and posterior variance once \\(\\tau\\) is fixed."
  },
  {
    "objectID": "posts/bayesian_logistic_regression/index.html#gauss-hermite-quadrature",
    "href": "posts/bayesian_logistic_regression/index.html#gauss-hermite-quadrature",
    "title": "Bayesian logistic regression with a fixed prior variance",
    "section": "Gauss-Hermite quadrature",
    "text": "Gauss-Hermite quadrature\nGauss-Hermite quadrature is a useful technique for numerical computations of integrals of the form\n\\[\\begin{align}\n\\int f(x) \\exp(-\\frac{1}{2} x^2) = \\sqrt{2 \\pi} \\mathbb E_{N(0, 1)}[f(X)]\n\\end{align}\\]\nThe quadrature rule is given by:\n\\[\\begin{align}\n\\int f(x) \\exp(-\\frac{1}{2} x^2) = \\sum_i^m f(x_i) w_i\n\\end{align}\\]\nWhere \\((x_i)_{i=1}^m\\) are the roots of the \\(m\\)-th Hermite polynomial. The corresponding weights \\(w\\) can be naively determined by solving a system of equations \\(Aw = b\\) where \\(A\\) is a matrix giving the evaluation of \\(m\\) functions \\((f_i)_{i=1}^m\\) at \\((x_i)_{i=1}^m\\) and \\(b\\) are there integrals \\(F_i = \\int f_i(x) e^{-x^2}\\) (there are more efficient ways to compute the weights, the point is that quadrature rules are essentially determined by their choice of evaluation points and weight function). The \\(f_i\\) must be polynomials of degree \\(2m+1\\), and the \\(m\\) point quadrature rule is exact for all polynomials degree \\(\\leq 2m+1\\), (why \\(2m+1\\)? consider that \\(f_i = \\sum_{i=0}^{2m} a_ix^i\\) the even terms get killed in the integral).\n\nChange of variable\nA simple change of variable let’s us use Gauss-Hermite quadrature to approximate expectations with respect to \\(N(\\mu, \\sigma^2)\\)\n\n\nRelationship to the Laplace approximation\nIf we take \\(\\mu = b^*\\) and \\(\\sigma = \\sqrt{\\frac{1}{\\lambda}}\\) the \\(1\\)-point Gauss-Hermite quadrature is equivalent to the Laplace approximation.\n\n\nDiscussion of efficiency\nTo compute the integral \\(\\int g(x) dx\\) using Gauss-Hermite quadrature we will factor out a Gaussian density \\(\\int \\frac{g(x)}{\\phi(x; \\mu, \\sigma^2)} \\phi(x; \\mu, \\sigma^2)\\). We see that there is a choice in which Gaussian density to factor out. The \\(m\\) point quadrature rule is exact for \\(h(x) = \\frac{g(x)}{\\phi(x; \\mu, \\sigma^2)}\\) a polynomial of degree \\(\\leq 2m+1\\). For our quadrature to be accurate we want \\(h(x)\\) to be well approximated by a low-degree polynomial. In particular, we care that the approximation is good in the regions of \\(\\mathbb R\\) that contribute most to the integral.\nIntegrals of this form a common in statistical applications.\n(LIU and PIERCE 1994) (Jäckel, n.d.)"
  },
  {
    "objectID": "posts/bayesian_logistic_regression/index.html#experiments",
    "href": "posts/bayesian_logistic_regression/index.html#experiments",
    "title": "Bayesian logistic regression with a fixed prior variance",
    "section": "Experiments",
    "text": "Experiments\n\nCode\n\nSimulation functions\n\n##### Minimal working example\nf <- paste0('', Sys.glob('cache/resources/C1*')) # clear cash so it can knit\nif(file.exists(f)){file.remove(f)}\n\nmake_c1_sim <- function(){\n  ##### Minimal working example\n  c1 <- gseasusie::load_msigdb_geneset_x('C1')\n\n  # sample random 5k background genes\n  set.seed(0)\n  background <- sample(rownames(c1$X), 5000)\n\n  # sample GSs to enrich, picked b0 so list is ~ 500 genes\n  enriched_gs <- sample(colnames(c1$X), 3)\n  b0 <- -2.2\n  b <- 3 *abs(rnorm(length(enriched_gs)))\n  logit <- b0 + (c1$X[background, enriched_gs] %*% b)[,1]\n  y1 <- rbinom(length(logit), 1, 1/(1 + exp(-logit)))\n  list1 <- background[y1==1]\n  X <- c1$X[background,]\n  X <- X[, Matrix::colSums(X) > 1]\n  idx <- which(colnames(X) %in% enriched_gs)\n\n  list(X = X, y = y1, idx = idx, b = b, b0 = b0, logits=logit)\n}\n\n\n\nSimulation functions\n\nlogsumexp <- function(x){\n  C <- max(x)\n  return(C + log(sum(exp(x - C))))\n}\n\nDiff <- function(f, g){Vectorize(function(b){f(b) - g(b)})}\nSum <- function(f, g){Vectorize(function(b){f(b) + g(b)})}\nExp <- logisticsusie:::Exp\nShift <- logisticsusie:::Shift\nProd <- function(f, g){Vectorize(function(b) f(b) * g(b))}\n\n\n\nExpand around MLE\n\n# compute log laplace bf expanded around the MLE\ncompute_laplace_lbf_mle <- function(betahat, shat2, lr, prior_variance){\n  tau1 <- 1/shat2\n  tau0 <- 1/prior_variance\n  tau <- tau1 + tau0\n  #lbf <- lr + 0.5 * log(tau1/tau) - 0.5  * tau1 * tau0 / tau * betahat^2\n  lbf <- lr + 0.5 * log(2 * pi/ tau1) + dnorm(betahat, mean=0, sd = sqrt(shat2 + prior_variance))\n  return(lbf)\n}\n\n#' Approximate log bf and posterior with normal approximation expanding about MLE\nlaplace_mle <- function(mle, prior_variance=1){\n  mle$lbf <- with(mle, compute_laplace_lbf_mle(betahat, shat2, lr, prior_variance))\n  \n  # add approximate posterior to mle\n  tau1 <- 1/mle$shat2\n  tau0 <- 1/prior_variance\n  mle$mu <- tau1/(tau0 + tau1) * mle$betahat\n  mle$tau <- tau1 + tau0\n  mle$prior_variance <- prior_variance\n  mle$logZ <- mle$lbf + mle$ll0\n  return(mle)\n}\n\n#' 'unshrink' a l2 regularized fit\nunshrink <- function(fit){\n  nu <- fit$tau\n  tau0 <- fit$tau0\n  tau <- nu - tau0\n  \n  betahat <-nu/tau * fit$mu\n  shat2 <- 1/tau\n  \n  # approximates the log liklihood under the mle\n  #ll <- fit$ll - 0.5 * log(tau0 / (2 * pi)) + 0.5 * nu*tau0/(nu - tau0) * fit$mu^2\n  \n  # this it the value of log likelihood that recovers the laplace MAP logZ\n  # should agree with formula above but maybe there seemes to be a subtle bug\n  tau1 <- 1/shat2\n  tau <- tau1 + tau0\n  ll <- fit$logZ - 0.5 * log(tau1/tau) + 0.5  * tau1 * tau0 / tau * betahat^2\n  ll0 <- fit$ll0\n  lr <- ll - ll0\n  return(list(betahat = betahat, shat2 = shat2, intercept=fit$intercept, ll0 = ll0, ll = ll, lr = lr))\n}\n\n#' 'unshrink' and 'reshrink'\nlaplace_map <- function(fit, prior_variance=1){\n  unsrhunk <- unshrink(fit)\n  return(laplace_mle(unsrhunk, prior_variance))\n}\n\n\n\nGauss-Hermite code\n\n#' gauss-hermite quadrature for \\int f(b) db\n#' centered on mu, with precision tau\ngauss_hermite <- function(f, mu, tau, ll0, m=9){\n  # make quadrature points\n  quad_points <- statmod::gauss.quad.prob(\n    m, dist='normal', mu=mu, sigma=sqrt(1/tau))\n  \n  # function logq(b)\n  logq <- function(b){\n    dnorm(b, mean=mu, sd=sqrt(1/tau), log=T) \n  }\n  \n  # compute marginal log likelihood\n  h <- Diff(f, logq)\n  \n  # compute log \\int f(b)/q(b) q(b) db\n  logZ <- logsumexp(h(quad_points$nodes) + log(quad_points$weights))\n  lbf <- logZ - ll0\n  \n  # compute posterior mean\n  h1 <- Prod(identity, Exp(Shift(h, logZ)))\n  mu <- sum(h1(quad_points$nodes) * quad_points$weights)\n  \n  # h2 <- Prod(function(x){x^2}, Exp(Shift(h, logZ)))\n  # mu2 <- sum(h2(quad_points$nodes) * quad_points$weights)\n  \n  h2 <- Sum(function(x){2 * abs(x)}, Shift(h, logZ))\n  logmu2 <- logsumexp(h2(quad_points$nodes) + log(quad_points$weights))\n  mu2 <- exp(logmu2)\n  var <- mu2 - mu^2\n  \n  list(lbf = lbf, logZ = logZ, mu=mu, var=var)\n}\n\nmake_log_joint <- function (x, y, o, beta0, sigma2) {\n    ll_base <- function(beta) {\n        psi <- beta0 + x * beta + o\n        ll <- sum(psi * y - log(1 + exp(psi))) + dnorm(beta, \n            sd = sqrt(sigma2), log = T)\n        return(ll)\n    }\n    log_joint <- Vectorize(ll_base)\n    return(log_joint)\n}\n\n#' fit has intercept, prior_variance, mu, tau, ll0\ngauss_hermite2 <- function(x, y, o, fit, m=9){\n  f <- with(fit, make_log_joint(x, y, o, intercept, prior_variance))\n  return(gauss_hermite(f, fit$mu, fit$tau, ll0, m = m))\n}\n\n\n\n\nExample\nHere is a simple example where \\(x_i \\sim N(0, 1)\\) and \\(p(y_i=1) = \\sigma(x_i)\\) (\\(b_0 = 0, b = 1\\)).\n\nx <- rnorm(1000)\ny <- rbinom(1000, 1, 1/(1 + exp(-x)))\no <- rep(0, length(y))\nll0 <- sum(dbinom(y, 1, mean(y), log=T))\nprior_variance <- 1\n\n\n# mle\nmle <- logisticsusie:::fit_fast_glm(x, y, rep(0, length(y)), ll0)\nmle <- laplace_mle(mle)\nmle$logZ\n\n[1] -590.8294\n\n# MAP prior variance = 1\n# this gives the MAP-based (standard) Laplace approximation\nfit1 <- logisticsusie:::ridge(x, y, o, prior_variance=1)\nfit1$logZ\n\n[1] -592.5539\n\ngauss_hermite2(x, y, o, fit1, m=1)$logZ # should agree with laplace\n\n[1] -592.5539\n\ngauss_hermite2(x, y, o, fit1, m=9)$logZ # note: odd number of nodes?\n\n[1] -592.5533\n\n# unshrink, reshrink\n# check that our \"unshrink\" operation works\nmle2 <- unshrink(fit1)\nfit11 <- laplace_mle(mle2, prior_variance = 1)\nfit11$logZ\n\n[1] -593.2808\n\n# MAP prior variance = 10\nfit10 <- logisticsusie:::ridge(x, y, o, prior_variance=10)\nfit10_1 <- laplace_map(fit10, prior_variance=1)\nfit10_1$logZ # expand around MAP for prior_variance = 10, approximate logZ for prior_variance=1\n\n[1] -594.4258\n\ngauss_hermite2(x, y, o, fit10_1, m=1)$logZ # slightly different, actually evaluates new point rather than approximate\n\n[1] -592.5508\n\ngauss_hermite2(x, y, o, fit10_1, m=9)$logZ\n\n[1] -592.5533\n\n# MAP prior variance = 1000, basically MLE\nfit1000 <- logisticsusie:::ridge(x, y, o, prior_variance=1000)\nfit1000_1 <- laplace_map(fit10, prior_variance=1)\nfit1000_1$logZ\n\n[1] -594.4258\n\ngauss_hermite2(x, y, o, fit1000_1, m=1)$logZ # slightly different, actually evaluates new point rather than approximate\n\n[1] -592.5508\n\ngauss_hermite2(x, y, o, fit1000_1, m=9)$logZ\n\n[1] -592.5533\n\n# MAP prior variance = 1e-10, basically null model\nfit1en10 <- logisticsusie:::ridge(x, y, o, prior_variance=1e-10)\nfit1en10_1 <- laplace_map(fit1en10, prior_variance=1)\nfit1en10_1$logZ\n\n[1] -592.7734\n\n# suprisingly, we can get some information out of the nearly zero effect estimate!\ngauss_hermite2(x, y, o, fit1en10_1, m=1)$logZ # slightly different, new expansion point given by MAP approximation\n\n[1] -595.9434\n\ngauss_hermite2(x, y, o, fit1en10_1, m=9)$logZ\n\n[1] -592.6643\n\ngauss_hermite2(x, y, o, fit1en10_1, m=17)$logZ\n\n[1] -592.6044\n\ngauss_hermite2(x, y, o, fit1en10_1, m=33)$logZ\n\n[1] -592.6037\n\n\n\n\nGauss-Hermite Quadrature\nGauss-Hermite quadrature is a natural choice for integrating over normal priors. We need to make a choice about where to “center” the prior\n\n\nGauss Hermite at the MAP\nIn this simulation the Laplace approximation is extremely accurate, and it only take 4 quadrature points to get a Gauss-Hermite quadrature rule that matches adaptive quadrature.\n\n# adaptive quadrature\nquad <- logisticsusie::logistic_bayes(x, y, prior_variance=prior_variance, width=Inf)\nquad$logZ\n\n[1] -592.5533\n\n# MAP prior variance = 1\nfit1 <- logisticsusie:::ridge(x, y, o, prior_variance=1)\nfit1$logZ == gauss_hermite2(x, y, o, fit1, m=1)$logZ # should agree with laplace\n\n[1] TRUE\n\npar(mfrow=c(1, 1))\ngh <- purrr::map_dbl(1:32, ~gauss_hermite2(x, y, o, fit1, m=.x)$logZ)\nplot(1:32, gh[1:32]); abline(h = quad$logZ, col='red')\n\n\n\n\n\n\nGauss-Hermite, approximate MAP\n\nMAP-MLE\nHere we first compute the MLE, and then approximate the MAP by taking a quadratic approximation of the likelihood and combining it with the prior. This approximation also gives us an approximation of the log-marginal likelihood (green), however this approximation must be distinguished from the true MAP approximation (blue), as well as the approximation yielded by expanding at the approximate MAP computed (\\(m=1\\)).\nWe see that the MLE-based MAP approximation is not very good. But the situation quickly improves even for the one-point Gauss-Hermite quadrature. The one point quadrature and Laplace approximation correspond when the single evaluation point is the mode, and the Gauss-Hermite quadrature rule is appropriately transformed to reflect the curvature at this point (LIU and PIERCE 1994). However, this does not hold at the approximate MAP. First, the approximation of the likelihood at the approximate MAP is only an approximation. Second, the \\(1\\)-point Gauss-Hermite quadrature does not quite correspond to the expansion at the approximate MAP, as the first order term in the approximation is assumed to be zero. For that to be true we must either be at the mode (so the gradient is 0) or the mean (so the expected value of the first order term is 0)$.\n\n# mle\nmle <- logisticsusie:::fit_fast_glm(x, y, rep(0, length(y)), ll0)\nmle <- laplace_mle(mle)\n\nmle$logZ\n\n[1] -590.8294\n\ngauss_hermite2(x, y, o, mle, m=1)$logZ # should agree with laplace\n\n[1] -592.5505\n\ngh <- purrr::map_dbl(1:32, ~gauss_hermite2(x, y, o, mle, m=.x)$logZ)\n\npar(mfrow=c(1, 2))\nplot(1:32, gh[1:32], ylim=range(gh, mle$logZ, fit1$logZ, quad$logZ));\nabline(h = quad$logZ, col='red');\nabline(h = fit1$logZ, col='blue');\nabline(h = mle$logZ, col='green');\n\nplot(1:32, gh[1:32], ylim=range(gh, fit1$logZ, quad$logZ));\nabline(h = quad$logZ, col='red');\nabline(h = fit1$logZ, col='blue');\n\n\n\n\n\n\nMAP-MAP\nHere we compute the MAP at some other value of the prior variance. We then “unshrink” the fit, and then “reshrink” it at the prior variance of interest.\n\nfit10 <- logisticsusie:::ridge(x, y, o, prior_variance=10)\nfit10_1 <- laplace_map(fit10, prior_variance=1)\n\nfit10_1$logZ\n\n[1] -594.4258\n\ngauss_hermite2(x, y, o, fit10_1, m=1)$logZ # should agree with laplace\n\n[1] -592.5508\n\ngh <- purrr::map_dbl(1:32, ~gauss_hermite2(x, y, o, fit10_1, m=.x)$logZ)\n\npar(mfrow=c(1, 2))\nplot(1:32, gh[1:32], ylim=range(gh, fit10_1$logZ, fit1$logZ, quad$logZ));\nabline(h = quad$logZ, col='red');\nabline(h = fit1$logZ, col='blue');\nabline(h = fit10_1$logZ, col='green');\n\nplot(1:32, gh[1:32], ylim=range(gh, fit1$logZ, quad$logZ));\nabline(h = quad$logZ, col='red');\nabline(h = fit1$logZ, col='blue');\n\n\n\n\nThe “unshrink” then “reshrink” procedure is probably better when we want to shrink more aggressively, but can be unreliable when we want to shrink less aggressively. In the extreme, we set the prior variance to a very small value. The MLE is way out in the tail of this Laplace approximation and our reconstruction can be very poor. Here we see even as we increase the number of quadrature points, the approximation fails to converge to the true value.\n\nfit1em5 <- logisticsusie:::ridge(x, y, o, prior_variance=1e-10)\nfit1em5_1 <- laplace_map(fit1em5, prior_variance=1)\n\nfit1em5_1$logZ\n\n[1] -592.7734\n\ngauss_hermite2(x, y, o, fit1em5_1, m=1)$logZ # should agree with laplace\n\n[1] -595.9434\n\ngh <- purrr::map_dbl(1:32, ~gauss_hermite2(x, y, o, fit1em5_1, m=.x)$logZ)\n\npar(mfrow=c(1, 2))\nplot(1:32, gh[1:32], ylim=range(gh, fit1em5$logZ, fit1$logZ, quad$logZ));\nabline(h = quad$logZ, col='red');\nabline(h = fit1$logZ, col='blue');\nabline(h = fit1em5$logZ, col='green');\n\nplot(1:32, gh[1:32], ylim=range(gh, fit1$logZ, quad$logZ));\nabline(h = quad$logZ, col='red');\nabline(h = fit1$logZ, col='blue');\n\n\n\n\n\n\n\nGauss-Hermite at the prior\nHere we explore Gauss-Hermite quadrature over the prior \\(q = N(0, \\sigma^2_0)\\). We see that (1) it takes a large number of quadrature points to get a good approximation of the log-marginal likelihood. and (2) the approximation error oscillates.\nNote that we fix the intercept at the MAP estimate of the intercept, so that we can expect as the quadrature points increase, the marginal likelihood should converge to the same result as adaptive quadrature and Gauss-Hermite at the MAP.\n\nnull_intercept <- log(mean(y)/mean(1-y))\nll <- sum(dbinom(y, 1, 1/(1 + exp(-quad$intercept)), log=T))\nnull <- list(mu = 0, prior_variance=1, tau0 = 1, intercept = quad$intercept, ll=ll, ll0 = ll0)\n\ngh_null <- purrr::map_dbl(1:128, ~gauss_hermite2(x, y, o, null, m=.x)$logZ)\n\npar(mfrow=c(1, 3))\nplot(1:128, gh_null[1:128]); abline(h = quad$logZ, col='red')\nplot(10:128, gh_null[10:128]); abline(h = quad$logZ, col='red')\nplot(100:128, gh_null[100:128]); abline(h = quad$logZ, col='red')"
  },
  {
    "objectID": "posts/bayesian_logistic_regression/index.html#dealing-with-the-intercept",
    "href": "posts/bayesian_logistic_regression/index.html#dealing-with-the-intercept",
    "title": "Bayesian logistic regression with a fixed prior variance",
    "section": "Dealing with the intercept",
    "text": "Dealing with the intercept\n\nFixed intercept\nIn the above demonstration we have worked with a fixed intercept.\nNotably, we still got very similar results for the MAP-MAP and MAP-MLE approaches, which used the intercept from the MAP under a different setting of the prior variance or the intercept from the MLE.\n\n\nProfile likelihood\nOne option is a profile likelihood approach. The idea is that we replace the likelihood \\(f(\\beta,\\beta_0)=\\log p(y | \\beta, \\beta_0)\\) with \\(g(\\beta) = \\max_{\\beta_0} f(\\beta, \\beta_0)\\). For example, under the null model (\\(\\beta = 0\\)) when there is no offset, we can easily compute the setting of the intercept that maximize \\(\\arg\\max_{\\beta_0}f(0, \\beta_0) - \\log \\frac{\\bar y}{1 - \\bar y}\\). I wonder if there is an analytic solution for the intercept when the offset/effects are non-zero. In this case we could cheaply update the intercept before marginalizing over \\(\\beta\\).\nOtherwise, it will take a few Newton steps (e.g. 5) to maximize the intercept, in which case we may be better off using those likelihood evaluations to integrate over the prior using a quadrature rule.\n\n\nIntegrating over the intercept\nWe consider integrating the intercept over a flat prior. For a given value of the effect, we would marginalize over the intercept with a Gauss-Hermite quadrature or a Laplace approximation. Here we provide code for computing the marginal likelihood where we integrate over the effect and intercept when there is a normal prior on the intercept. We set the prior variance to a large value, \\(\\sigma^2_{int} = 1000\\), so that it is effectively a flat prior.\n\nintegrate_intercept <- function(x, y, o, prior_variance=1, m_int=5, m=5){\n  # 1. fit MAP\n  tau0 <- 1/prior_variance\n  ridgefit <- logisticsusie:::ridge(x, y, o, prior_variance=1/tau0)\n  \n  # 2. make function that marginalized over the intercept (flat prior)\n  intercept_marginalized <- function(b){\n    # log p(y, b | b0)\n    f1 <- Vectorize(function(b0){\n      psi <- b0 + x * b\n      if (!is.null(o)) {\n          psi <- psi + o\n      }\n      return(sum((y * psi) - log(1 + exp(psi))) + dnorm(b, 0, \n          sd = sqrt(1/tau0), log = T) + dnorm(b0, 0, sd=10000, log=T))\n    })\n    \n    # adaptive quadrature\n    if(m_int == -1){\n      # log p(y, b | b0) - logp(y, b_MAP | b0_MAP)\n      C <- f1(ridgefit$intercept)\n      f2 <- Shift(f1, C)\n      I <- integrate(Exp(f2), -Inf, Inf)\n      logZ <- log(I$value) + C\n    } else{\n      # gauss-hermite, center at MAP\n      b0 <- ridgefit$intercept\n      psi <- b0 + x * b + o\n      mu <- 1/(1 + exp(-psi))\n      tau <- sum(mu * (1 - mu))\n      \n      # function logq(b)\n      logq <- function(x){\n        dnorm(x, mean=b0, sd=sqrt(1/tau), log=T) \n      }\n      h <- Diff(f1, logq)\n      quad_points <- statmod::gauss.quad.prob(m_int, dist='normal', mu=b0, sigma=sqrt(1/tau))\n      logZ <- logsumexp(h(quad_points$nodes) + log(quad_points$weights))\n      logZ\n    }\n  return(logZ)\n  }\n  \n  # 3. compute marginal likelihood\n  C <- intercept_marginalized(ridgefit$mu)\n  f2 <- Shift(intercept_marginalized, C)\n  \n  # function logq(b)\n  logq <- function(x){\n    dnorm(x, mean=ridgefit$mu, sd=sqrt(1/ridgefit$tau), log=T) \n  }\n  h <- Diff(intercept_marginalized, logq)\n  quad_points <- statmod::gauss.quad.prob(m, dist='normal', mu=ridgefit$mu, sigma=sqrt(1/ridgefit$tau))\n  logZ <- logsumexp(h(quad_points$nodes) + log(quad_points$weights))\n  return(logZ)\n}\n\nWe see close agreement between the 1 x 1 and 64 x 64 Gauss-Hermite quadrature rules. This suggests that we can effectively integrate over both the intercept and the prior with just a few likelihood evaluations.\nNote that when we use the Laplace approximation (or \\(1\\)-point Gauss-Hermite quadrature) we effectively scale the fixed-intercept BFs by \\(\\sqrt{\\frac{2\\pi}{\\tau_{int}}}\\). where \\(\\tau_{int} = \\frac{d^2}{d\\beta_0^2} \\log p(y, \\hat \\beta, \\beta_0 | x) \\vert_{\\beta_0 = \\hat \\beta_0}\\).\nConsider this nested 1d quadrature scheme. In the \\(1 \\times 1\\) case it seems to correspond to a “diagonalized” Laplace approximation. In particular it does not take into account the dependence between the intercept and effect.\nPerhaps the correct thing to do is a multivariate Gauss-Hermite quadrature (Jäckel, n.d.). This is an obvious extension to the 1d Gauss-Hermite quadrature rule, and it would allow us to more accurately deal with the dependence between the intercept and effect estimates. As discuessed in the reference, different decompositions of the covariance matrix will yield different quadrature rules.But basically, you should think of taking the isotropic 2d quadrature rule and rotating + stretching it to align it with a local model of the curvature at the mode.\nIt’s worth noting that the cost of computing a decomposition of a \\(2 \\times 2\\) matrix is negligible compared to the cost of evaluating the likelihood at a single point. It probably makes sense to invest in taking this step because it allows us to better invest our our evaluation points.\nIt is satisfying also, that we restore our relationship between the (multivariate) Gauss-Hermite and the (multivariate) Laplace approximation in the \\(1\\)-point quadrature rule (TODO: confirm this!).\n\nintegrate_intercept(x, y, o, 1, m_int=1, m=1)\n\n[1] -604.4207\n\nintegrate_intercept(x, y, o, 1, m_int=-1, m=1)\n\n[1] -604.4205\n\nintegrate_intercept(x, y, o, 1, m_int=1, m=64)\n\n[1] -604.4195\n\nintegrate_intercept(x, y, o, 1, m_int=-1, m=64)\n\n[1] -604.4192\n\nintegrate_intercept(x, y, o, 1, m_int=3, m=3)\n\n[1] -604.4197\n\nintegrate_intercept(x, y, o, 1, m_int=5, m=5)\n\n[1] -604.4192\n\nintegrate_intercept(x, y, o, 1, m_int=1, m=5)\n\n[1] -604.4195\n\nintegrate_intercept(x, y, o, 1, m_int=1, m=5)\n\n[1] -604.4195\n\nintegrate_intercept(x, y, o, 1, m_int=64, m=64)\n\n[1] -604.4192\n\nintegrate_intercept(x, y, o, 1, m_int=128, m=128)\n\n[1] -604.4192\n\n\n\n\nMultivariate Gauss-Hermite\n\\(\\Sigma = LL^T\\) is a covariance matrix and it’s corresponding Cholesky decomposition. So that if \\({\\bf z} \\sim N(0, I)\\) then \\(L {\\bf z} + \\mu \\sim N(\\mu, \\Sigma)\\). With \\({\\bf b}({\\bf z}) = L {\\bf z} + \\mu\\)\n\\[\\begin{align}\np(y)\n&= \\int_{\\mathbb R^2} \\frac{p(y, {\\bf b})}{q({\\bf b})} q({\\bf b}) d{\\bf b} \\\\\n&= \\int_{\\mathbb R^2} \\frac{p(y, {\\bf b})}{\\exp\\{-\\frac{1}{2} ({\\bf b} - \\mu)^T \\Sigma^{-1} ({\\bf b}-\\mu) \\}} \\exp\\{-\\frac{1}{2} ({\\bf b} - \\mu)^T \\Sigma^{-1} ({\\bf b}-\\mu) \\} d{\\bf b} \\\\\n&= \\det L \\int_{\\mathbb R^2} \\frac{p(y, {\\bf b}({\\bf z}))}{\\exp\\{-\\frac{1}{2} {\\bf z}^T {\\bf z} \\}} \\exp\\{-\\frac{1}{2} {\\bf z}^T {\\bf z} \\} d{\\bf z}\n\\end{align}\\]\n\nx <- rnorm(1000)\ny <- rbinom(1000, 1, 1/(1 + exp(-x)))\no <- rep(0, length(y))\nll0 <- sum(dbinom(y, 1, mean(y), log=T))\nprior_variance <- 1\n\nmap <- logisticsusie:::ridge(x, y, o, prior_variance=1)\nX0 <- cbind(rep(1, length(x)), x)\npsi <- map$intercept + x * map$mu + o\nmu <- 1/(1 + exp(-psi)) \ng <- t(X0) %*% (y - mu) + c(0, map$mu/prior_variance)\nH <- -t(X0) %*% (mu * (1 - mu) * X0) - diag(c(0, 1/prior_variance))\n\n# posterior precision is -H\n# LL^T = Sigma = -H^{-1}\nL <- solve(chol(-H))\nmu <- c(0, map$mu)\n\nf1 <- Vectorize(function(b0, b1){\n  psi <- b0 + x * b1 + o\n  return(sum((y * psi) - log(1 + exp(psi))) + dnorm(b1, 0, sd = sqrt(prior_variance), log = T) + dnorm(b0, 0, sd=10000, log=T))\n})\n\nf2 <- Vectorize(function(z1, z2){\n  z <- c(z1, z2)\n  beta <- sqrt(2) * (L%*%z)[,1] + mu\n  f1(beta[1], beta[2]) + 0.5 * sum(z*z)\n})\n\nC <- f1(map$intercept, map$mu)\n\nm <- 9\nquad_points <- statmod::gauss.quad.prob(m, dist='normal', mu=0, sigma=1)\nquad_points <- statmod::gauss.quad(m, kind='hermite')\n\n\nz1 <- rep(quad_points$nodes, m)\nz2 <- rep(quad_points$nodes, each=m)\nZ <- cbind(z1, z2)\nw <- rep(quad_points$weights, m) * rep(quad_points$weights, each=m)\n\nlogZ <- sum(log(diag(L))) + logsumexp(f2(z1, z2) + log(w))\nlogZ\n\n[1] -589.2138\n\nintegrate_intercept(x, y, o, 1, m_int=9, m=9)\n\n[1] -587.7452\n\nmle <- logisticsusie:::fit_fast_glm(x, y, rep(0, length(y)), ll0)\n\n\n\n# -H^{-1} = Q %*% t(Q)\nQ <- t(with(svd(-H), diag(1/sqrt(d)) %*% t(v)))\nquad_points <- statmod::gauss.quad.prob(9, dist='normal', mu=0, sigma=1)\n\n# make a grid of x, y points where x and y both take values from quad_points$nodes\nm = 16\nz1 <- rep(quad_points$nodes, m)\nz2 <- rep(quad_points$nodes, each=m)\n\n# get evaluation points\nQz <- Q %*% rbind(z1, z2)\nx2 <- Qz[1,] + mle$intercept\ny2 <- Qz[2,] + mle$betahat\n\nf1 <- Vectorize(function(b0, b1){\n    psi <- b0 + x * b1 + o\n    return(sum((y * psi) - log(1 + exp(psi)))) \n    # + dnorm(b, 0, sd = sqrt(1/tau0), log = T) + dnorm(b0, 0, sd=10000, log=T))\n  })\n\n#w <- rep(quad_points$weights, m) * rep(quad_points$weights, each=m) - 0.5 * log(det(-H)) + 0.5 * nrow(H) * log(2*pi) +  logsumexp(f1(x2, y2) - C + log(w))\n\n\n#logsumexp(f1(x2, y2) + log(w))\n\n\nC <- f1(mle$intercept, mle$betahat)\n\nf2 <- function(b1){Vectorize(function(b0){f1(b0, b1)})}\n\nf3 <- Vectorize(function(b1){\n  g <- f2(b1)\n  g2 <- Exp(Shift(g, C))\n  return(log(integrate(g2, -Inf, Inf)$value) + C)\n})\n\nh <- Exp(Shift(f3, C))\nlog(integrate(h, -Inf, Inf)$value) + C\n\n[1] -575.9966\n\n\n\nintegrate_intercept(x, y, o, 1, m_int=64, m=64)\n\n[1] -587.7452\n\npar(mfrow=c(1,2))\nplot(z1, z2)\nplot(x2, y2)\n\n\n\n\n\nridgefit <- logisticsusie:::ridge(x, y, o, prior_variance=1)\nf1 <- log_joint2(x, y, o, 1)\n\nb <- ridgefit$mu + 4\nf2 <- function(b0){f1(b=b, b0)}\n# adaptive quadrature\nf3 <- Shift(f2, ridgefit$ll)\nlog(integrate(Exp(f3), -Inf, Inf)$value)\nI <- integrate(Exp(f3), -Inf, Inf)\nlog(I$value) + ridgefit$ll\n\n# gauss-hermite\nb0 <- ridgefit$intercept\npsi <- b0 + x * b\nmu <- 1/(1 + exp(-psi))\ntau <- sum(mu * (1 - mu))\n\n# function logq(b)\nlogq <- function(x){\n  dnorm(x, mean=b0, sd=sqrt(1/tau), log=T) \n}\nh <- Diff(f3, logq)\nquad_points <- statmod::gauss.quad.prob(9, dist='normal', mu=b0, sigma=sqrt(1/tau))\nlogZ <- logsumexp(h(quad_points$nodes) + log(quad_points$weights)) + ridgefit$ll\nlogZ\n\ngauss_hermite(f3, b0, var, 0)\n\nlog(sum(exp(f3(quad_points$nodes)) * quad_points$weights))\n\nplot(f3, xlim=c(-0.5, 0.5))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "generalizing_susie",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 24, 2024\n\n\nBayesian logistic regression with a fixed prior variance\n\n\nKarl Tayeb\n\n\n\n\nFeb 22, 2024\n\n\nProfile likelihood for logistic regression\n\n\nKarl Tayeb\n\n\n\n\nFeb 8, 2024\n\n\nWakefield’s ABF vs Laplace approximation for variable selection with SuSiE\n\n\nKarl Tayeb\n\n\n\n\nFeb 8, 2024\n\n\nFine-mapping case contral GWAS\n\n\nKarl Tayeb\n\n\n\n\nJan 25, 2024\n\n\nOne variable at a time is better for inference in SER\n\n\nKarl Tayeb\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]